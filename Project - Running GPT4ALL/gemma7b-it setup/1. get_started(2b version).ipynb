{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup the pytorch/CUDA/GPU environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 15.74 GB\n",
      "Used GPU Memory: 0.00 GB\n",
      "Free GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"Used GPU Memory: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "        print(f\"Free GPU Memory: {torch.cuda.memory_reserved(0) / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the workstation has 16GB VRAM. \n",
    "A simple \"divide by 4\" rule of thumb, we can run at most 4B parameters model.\n",
    "\n",
    "The configurarion of this model can be found on: https://huggingface.co/docs/transformers/en/model_doc/gemma#transformers.GemmaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is available in this environment!\n",
      "Number of GPUs available: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clear any cached memory (might help in some cases)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU support) is available in this environment!\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    # Get the name of the GPU\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/researcher/.pyenv/versions/3.11.7/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "# Ensure no gradients are computed for the model (saves memory)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\").eval()\n",
    "\n",
    "# Check if a GPU is available and move the model to GPU if it is\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>write me a python code that generate a simple game board with a set number of pieces.\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "# Create a board with 10x10 pieces\n",
      "board = [[0 for _ in range(10)] for _ in range(10)]\n",
      "\n",
      "# Randomly place pieces on the board\n",
      "for i in range(10):\n",
      "    for j in range(10):\n",
      "        if random.random() < 0.5:\n",
      "            board[i][j] = 1\n",
      "\n",
      "# Print the board\n",
      "for row in board:\n",
      "    print(row)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* `import random` imports the `random` module, which provides functions to generate random numbers.\n",
      "* `board = [[0 for _ in range(10)] for _ in range(10)]` creates a 10x10 board with all elements initialized to 0.\n",
      "* The `for` loops iterate over each element in the board.\n",
      "* `random.random() < 0.5` generates a random number between 0 and 0.5. If the random number is less than 0.5, it sets the corresponding element to 1.\n",
      "* `print(board)` prints the board, row by row.\n",
      "\n",
      "**Output:**\n",
      "\n",
      "The code will generate a random game board with 10x10 pieces, where each element is either 0 or 1.\n",
      "\n",
      "**Example Output:**\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text and move the tensor to GPU if available\n",
    "input_text = \"write me a python code that generate a simple game\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = input_ids.to(\"cuda\") if torch.cuda.is_available() else input_ids\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**input_ids, max_new_tokens = 300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model has already successfully setup on the PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Skipped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
